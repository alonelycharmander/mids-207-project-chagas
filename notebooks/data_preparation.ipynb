{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2ab0b0",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "Convert all raw ECG records into standardized NumPy arrays (400 Hz, 7.3 s, lead-wise z-score), build the train, validation, and external-test splits, and save each split as a compressed `.npz` file for fast loading in model notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71658b0c",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce498811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.signal import resample\n",
    "from sklearn.utils import resample as resample_bal \n",
    "import ecgmentations as E\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from helper_code import (\n",
    "    find_records, load_header, load_signals, load_label,\n",
    "    reorder_signal, get_age, get_sex, get_source\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b45b36a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will preprocess 19,916 records from ['exams_part6']\n",
      "CODE-15 % records : 19,916\n",
      "SaMi-Trop records : 1,631\n",
      "PTB-XL   records  : 21,799\n",
      "\n",
      "CODE-15 example   : ['exams_part6\\\\1000175', 'exams_part6\\\\1000190', 'exams_part6\\\\1000201']\n",
      "SaMi-Trop example : ['100726', '101191', '101193']\n",
      "PTB-XL example    : ['00000\\\\00001_hr', '00000\\\\00002_hr', '00000\\\\00003_hr']\n"
     ]
    }
   ],
   "source": [
    "# Root folder containing the three datasets\n",
    "DATA_ROOT = 'data'\n",
    "\n",
    "CODE15_DIR   = os.path.join(DATA_ROOT, 'code15_output')\n",
    "SAMITROP_DIR = os.path.join(DATA_ROOT, 'samitrop_output')\n",
    "PTBXL_DIR    = os.path.join(DATA_ROOT, 'ptbxl_output')\n",
    "\n",
    "# Choose the datasets to process\n",
    "PARTS_TO_PROCESS = ['exams_part6']        # e.g. ['exams_part1'] or ['exams_part0', 'exams_part1']\n",
    "code15_records   = find_records(CODE15_DIR)\n",
    "code15_records = [\n",
    "    rid for rid in code15_records\n",
    "    if any(rid.startswith(p) for p in PARTS_TO_PROCESS)\n",
    "]\n",
    "print(f'Will preprocess {len(code15_records):,} records from {PARTS_TO_PROCESS}')\n",
    "\n",
    "samitrop_records = find_records(SAMITROP_DIR)\n",
    "ptbxl_records    = find_records(PTBXL_DIR)\n",
    "\n",
    "# Count WFDB records in each dataset\n",
    "print(f'CODE-15 % records : {len(code15_records):,}')\n",
    "print(f'SaMi-Trop records : {len(samitrop_records):,}')\n",
    "print(f'PTB-XL   records  : {len(ptbxl_records):,}\\n')\n",
    "\n",
    "# Show a few file names from each folder\n",
    "print(f'CODE-15 example   : {code15_records[:3]}')\n",
    "print(f'SaMi-Trop example : {samitrop_records[:3]}')\n",
    "print(f'PTB-XL example    : {ptbxl_records[:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c13b7",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358407bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_ecg(record_path, target_fs=400, target_len=2920):\n",
    "    \"\"\"Resample to target_fs, trim/pad to target_len, z-score each lead.\"\"\"\n",
    "    # Load raw signal and metadata\n",
    "    signal, fields = load_signals(record_path)\n",
    "    ref_leads = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF',\n",
    "                 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    signal = reorder_signal(signal, fields['sig_name'], ref_leads)\n",
    "\n",
    "    # 1) Resample if needed\n",
    "    in_fs = fields['fs']\n",
    "    if in_fs != target_fs:\n",
    "        n_samples = int(signal.shape[0] * target_fs / in_fs)\n",
    "        signal = resample(signal, n_samples, axis=0)\n",
    "\n",
    "    # 2) Trim or zero-pad to fixed length\n",
    "    if signal.shape[0] > target_len:\n",
    "        signal = signal[:target_len, :]\n",
    "    elif signal.shape[0] < target_len:\n",
    "        pad = np.zeros((target_len - signal.shape[0], signal.shape[1]))\n",
    "        signal = np.vstack([signal, pad])\n",
    "\n",
    "    # 3) Lead-wise z-score normalization\n",
    "    mu = np.nanmean(signal, axis=0)\n",
    "    sigma = np.nanstd(signal, axis=0) + 1e-8\n",
    "    signal = (signal - mu) / sigma\n",
    "\n",
    "    return signal.astype(np.float32)  # shape: (2920, 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969be083",
   "metadata": {},
   "source": [
    "### Preprocess CODE-15 % records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a76f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/19916 records\n",
      "Processed 2000/19916 records\n",
      "Processed 3000/19916 records\n",
      "Processed 4000/19916 records\n",
      "Processed 5000/19916 records\n",
      "Processed 6000/19916 records\n",
      "Processed 7000/19916 records\n",
      "Processed 8000/19916 records\n",
      "Processed 9000/19916 records\n",
      "Processed 10000/19916 records\n",
      "Processed 11000/19916 records\n",
      "Processed 12000/19916 records\n",
      "Processed 13000/19916 records\n",
      "Processed 14000/19916 records\n",
      "Processed 15000/19916 records\n",
      "Processed 16000/19916 records\n",
      "Processed 17000/19916 records\n",
      "Processed 18000/19916 records\n",
      "Processed 19000/19916 records\n",
      "Processed 19916/19916 records\n",
      "Final CODE-15 % array shape : (19916, 2920, 12)\n",
      "Number of positives         : 408\n",
      "Saved code15_part6_proc.npz\n"
     ]
    }
   ],
   "source": [
    "# Lists for the preprocessed signals and their labels\n",
    "X_code15, y_code15 = [], []\n",
    "\n",
    "for i, rid in enumerate(code15_records, 1):\n",
    "    # Build absolute path to WFDB record\n",
    "    rec_path = os.path.join(CODE15_DIR, rid)\n",
    "\n",
    "    # Apply resample → trim/pad → z-score to get (2920, 12) array\n",
    "    X_code15.append(preprocess_ecg(rec_path))\n",
    "\n",
    "    # Load binary Chagas label (True = positive, False = negative)\n",
    "    y_code15.append(load_label(rec_path))\n",
    "\n",
    "    # Progress update every 1000 records\n",
    "    if i % 1000 == 0 or i == len(code15_records):\n",
    "        print(f'Processed {i}/{len(code15_records)} records')\n",
    "\n",
    "# Stack all signals into one NumPy array (N, 2920, 12) and labels into (N,)\n",
    "X_code15 = np.stack(X_code15).astype(np.float32)\n",
    "y_code15 = np.array(y_code15, dtype=bool)\n",
    "\n",
    "print(f'Final CODE-15 % array shape : {X_code15.shape}')\n",
    "print(f'Number of positives         : {y_code15.sum()}')\n",
    "\n",
    "# Save to disk so future notebooks can load in seconds\n",
    "np.savez_compressed('code15_part6_proc.npz', X=X_code15, y=y_code15) # rename if more code15 parts are added\n",
    "print('Saved code15_part6_proc.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8db854",
   "metadata": {},
   "source": [
    "### Train–validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "518b00a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded code15_part0_proc.npz (19901, 2920, 12)\n",
      "Loaded code15_part1_proc.npz (19897, 2920, 12)\n",
      "Loaded code15_part2_proc.npz (19902, 2920, 12)\n",
      "Loaded code15_part3_proc.npz (19915, 2920, 12)\n",
      "Loaded code15_part4_proc.npz (19919, 2920, 12)\n",
      "Loaded code15_part5_proc.npz (19912, 2920, 12)\n",
      "Loaded code15_part6_proc.npz (19916, 2920, 12)\n",
      "Combined CODE-15 % shape: (139362, 2920, 12)\n",
      "Saved code15_parts_0-6_proc.npz\n"
     ]
    }
   ],
   "source": [
    "# Load every pre-processed CODE-15 % part and concatenate\n",
    "code15_parts = sorted(glob.glob('code15_part*_proc.npz')) \n",
    "X_list, y_list = [], []\n",
    "for f in code15_parts:\n",
    "    d = np.load(f)\n",
    "    X_list.append(d['X'])\n",
    "    y_list.append(d['y'])\n",
    "    print('Loaded', f, d['X'].shape)\n",
    "    \n",
    "# Concatenate all parts into one array\n",
    "X_code15 = np.concatenate(X_list)\n",
    "y_code15 = np.concatenate(y_list)\n",
    "print('Combined CODE-15 % shape:', X_code15.shape)\n",
    "\n",
    "# Save the combined array\n",
    "np.savez_compressed('code15_parts_0-6_proc.npz', X=X_code15, y=y_code15)\n",
    "print('Saved code15_parts_0-6_proc.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (111489, 2920, 12), positives: 2235\n",
      "Val shape   : (27873, 2920, 12), positives: 559\n",
      "Balanced train shape : (8940, 2920, 12), positives: 2235\n",
      "Saved train_balanced.npz and val_internal.npz\n"
     ]
    }
   ],
   "source": [
    "# Stratified 80 / 20 split for train and internal validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_code15, y_code15,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_code15)\n",
    "\n",
    "print(f'Train shape : {X_train.shape}, positives: {y_train.sum()}')\n",
    "print(f'Val shape   : {X_val.shape}, positives: {y_val.sum()}')\n",
    "\n",
    "# Balance the training set: keep all positives and sample 3 negatives per positive\n",
    "pos_idx = np.where(y_train)[0]\n",
    "neg_idx = np.where(~y_train)[0]\n",
    "\n",
    "neg_sample = resample_bal(\n",
    "    neg_idx,\n",
    "    replace=False,\n",
    "    n_samples=len(pos_idx) * 3,\n",
    "    random_state=42)\n",
    "\n",
    "balanced_idx = np.hstack([pos_idx, neg_sample])\n",
    "\n",
    "X_bal = X_train[balanced_idx]\n",
    "y_bal = y_train[balanced_idx]\n",
    "\n",
    "print(f'Balanced train shape : {X_bal.shape}, positives: {y_bal.sum()}')\n",
    "\n",
    "# Save the training and validation sets\n",
    "np.savez_compressed('train_full.npz',     X=X_train, y=y_train)   # full, imbalanced train set\n",
    "np.savez_compressed('train_balanced.npz', X=X_bal, y=y_bal)       # undersampled train set\n",
    "np.savez_compressed('val.npz', X=X_val, y=y_val)                  # untouched validation set\n",
    "print('Saved train_full.npz, train_balanced.npz, and val.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bcb5bb",
   "metadata": {},
   "source": [
    "### Augment training data\n",
    "\n",
    "To improve generalization and robustness, we apply data augmentation to the training set using the `ecgmentations` library. These transformations simulate realistic ECG artefacts commonly encountered in clinical settings. Augmentation helps the model learn to focus on disease-related signal morphology rather than irrelevant noise patterns. We apply these transformations only to the training data, validation and test sets remain untouched to ensure unbiased evaluation.\n",
    "\n",
    "The augmentation pipeline includes the following components:\n",
    "\n",
    "- `GaussNoise`: Adds mild to moderate broadband electronic noise to the waveform, simulating real-world electrical interference. We apply this with a 30% probability using a fixed noise variance of 0.005.\n",
    "- `AmplitudeScale`: Randomly scales the amplitude of each ECG signal by ±20% to mimic differences in gain settings or skin–electrode impedance.\n",
    "- `PowerlineNoise`: Injects sinusoidal noise resembling 60 Hz mains hum, which frequently contaminates ECG recordings. This is applied with 20% probability and capped at a low amplitude to preserve waveform readability.\n",
    "- `TimeShift`: Applies a temporal shift of up to ±7% of the signal length (≈±200 samples) to simulate misalignment or onset jitter between leads, improving the model’s tolerance to temporal variability.\n",
    "- `SinePulse`: Adds low-frequency baseline drift (0.5–2 Hz) to simulate respiration-related or motion artefacts that often appear in ambulatory ECGs.\n",
    "\n",
    "These parameter values were chosen to maintain clinical realism while encouraging robustness. More extreme augmentations were avoided to preserve essential waveform characteristics like P-waves and QRS complexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "379493ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8940, 2920, 12)\n",
      "y shape: (8940,) positives: 2235\n"
     ]
    }
   ],
   "source": [
    "# Load the training set, if needed\n",
    "DATA_DIR = 'data/prepared'\n",
    "\n",
    "train_full = np.load(f'{DATA_DIR}/train_balanced_parts0-6.npz')\n",
    "X_train, y_train = train_full['X'], train_full['y']   # rename as needed\n",
    "\n",
    "print('X shape:', X_train.shape)\n",
    "print('y shape:', y_train.shape, 'positives:', y_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fcecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented train shape: (17880, 2920, 12), positives: 4470\n",
      "Saved train_full_aug.npz\n"
     ]
    }
   ],
   "source": [
    "# Build augmentation pipeline\n",
    "augment = E.Sequential([\n",
    "    # Gaussian broadband noise\n",
    "    E.GaussNoise(\n",
    "        mean=0.0,\n",
    "        variance=0.005,        # noisier than default (0.01 would be very loud)\n",
    "        per_channel=True,\n",
    "        p=0.30\n",
    "    ),\n",
    "    # Amplitude scaling  ±20 %  (default is ±5 %)\n",
    "    E.AmplitudeScale(\n",
    "        scaling_range=(-0.20, 0.20),\n",
    "        p=0.30\n",
    "    ),\n",
    "    # 50/60 Hz mains hum   (amplitude limited to 0.2 mV)\n",
    "    E.PowerlineNoise(\n",
    "        ecg_frequency=400,          # our resampled ECG rate\n",
    "        powerline_frequency=60,     # US mains\n",
    "        amplitude_limit=0.20,\n",
    "        p=0.20\n",
    "    ),\n",
    "    # Temporal shift  ±0.07 fraction ≈ ±200 samples at 2 920-sample trace\n",
    "    E.TimeShift(\n",
    "        shift_limit=0.07,\n",
    "        p=0.30\n",
    "    ),\n",
    "    # Low-frequency baseline wander 0.5–2 Hz\n",
    "    E.SinePulse(\n",
    "        ecg_frequency=400,\n",
    "        pulse_frequency_range=(0.5, 2.0),\n",
    "        amplitude_limit=0.50,\n",
    "        p=0.20\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Apply once to every sample in the balanced-train set\n",
    "X_aug = np.empty_like(X_train)\n",
    "for i, ecg in enumerate(X_train):\n",
    "    X_aug[i] = augment(ecg=ecg)['ecg']     # returns dict with key 'ecg'\n",
    "\n",
    "# Combine originals + augmented copies\n",
    "X_train_aug = np.concatenate([X_train, X_aug])\n",
    "y_train_aug = np.concatenate([y_train, y_train])\n",
    "\n",
    "print(f'Augmented train shape: {X_train_aug.shape}, '\n",
    "      f'positives: {y_train_aug.sum()}')\n",
    "\n",
    "# Save for reuse\n",
    "np.savez_compressed('train_bal_parts0-6_aug.npz', X=X_train_aug, y=y_train_aug)\n",
    "print('Saved train_bal_parts0-6_aug.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cfc389",
   "metadata": {},
   "source": [
    "### Preprocess SaMi-Trop (positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56b34a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/1631 SaMi-Trop records\n",
      "Processed 1000/1631 SaMi-Trop records\n",
      "Processed 1500/1631 SaMi-Trop records\n",
      "Processed 1631/1631 SaMi-Trop records\n",
      "SaMi-Trop array shape: (1631, 2920, 12)\n",
      "Saved samitrop_proc.npz\n"
     ]
    }
   ],
   "source": [
    "# Convert every SaMi-Trop ECG to a (2920, 12) array\n",
    "X_pos = []\n",
    "for i, rid in enumerate(samitrop_records, 1):\n",
    "    rec_path = os.path.join(SAMITROP_DIR, rid)\n",
    "    X_pos.append(preprocess_ecg(rec_path))\n",
    "\n",
    "    # Progress update every 500 records\n",
    "    if i % 500 == 0 or i == len(samitrop_records):\n",
    "        print(f'Processed {i}/{len(samitrop_records)} SaMi-Trop records')\n",
    "\n",
    "# Stack signals and create label vector (all positives)\n",
    "X_pos = np.stack(X_pos).astype(np.float32)\n",
    "y_pos = np.ones(len(X_pos), dtype=bool)\n",
    "print(f'SaMi-Trop array shape: {X_pos.shape}')\n",
    "\n",
    "# Save for reuse\n",
    "np.savez_compressed('samitrop_proc.npz', X=X_pos, y=y_pos)\n",
    "print('Saved samitrop_proc.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e744a6f",
   "metadata": {},
   "source": [
    "### Preprocess PTB-XL (negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25139822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_0.npy  shape (1000, 2920, 12)\n",
      "Processed 2000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_1.npy  shape (1000, 2920, 12)\n",
      "Processed 3000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_2.npy  shape (1000, 2920, 12)\n",
      "Processed 4000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_3.npy  shape (1000, 2920, 12)\n",
      "Processed 5000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_4.npy  shape (1000, 2920, 12)\n",
      "Processed 6000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_5.npy  shape (1000, 2920, 12)\n",
      "Processed 7000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_6.npy  shape (1000, 2920, 12)\n",
      "Processed 8000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_7.npy  shape (1000, 2920, 12)\n",
      "Processed 9000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_8.npy  shape (1000, 2920, 12)\n",
      "Processed 10000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_9.npy  shape (1000, 2920, 12)\n",
      "Processed 11000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_10.npy  shape (1000, 2920, 12)\n",
      "Processed 12000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_11.npy  shape (1000, 2920, 12)\n",
      "Processed 13000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_12.npy  shape (1000, 2920, 12)\n",
      "Processed 14000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_13.npy  shape (1000, 2920, 12)\n",
      "Processed 15000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_14.npy  shape (1000, 2920, 12)\n",
      "Processed 16000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_15.npy  shape (1000, 2920, 12)\n",
      "Processed 17000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_16.npy  shape (1000, 2920, 12)\n",
      "Processed 18000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_17.npy  shape (1000, 2920, 12)\n",
      "Processed 19000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_18.npy  shape (1000, 2920, 12)\n",
      "Processed 20000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_19.npy  shape (1000, 2920, 12)\n",
      "Processed 21000/21799 PTB-XL records\n",
      "Saved ptbxl_batch_20.npy  shape (1000, 2920, 12)\n",
      "Processed 21799/21799 PTB-XL records\n",
      "Saved ptbxl_batch_21.npy  shape (799, 2920, 12)\n",
      "Final PTB-XL array shape: (21799, 2920, 12)\n",
      "Saved ptbxl_proc.npz\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1000        # number of ECGs per temporary file\n",
    "batch_X, temp_files = [], []\n",
    "batch_id = 0\n",
    "\n",
    "for i, rid in enumerate(ptbxl_records, 1):\n",
    "    rec_path = os.path.join(PTBXL_DIR, rid)\n",
    "    batch_X.append(preprocess_ecg(rec_path))       # (2920, 12) float32\n",
    "\n",
    "    # Progress update every 1 000 records\n",
    "    if i % 1000 == 0 or i == len(ptbxl_records):\n",
    "        print(f'Processed {i}/{len(ptbxl_records)} PTB-XL records')\n",
    "\n",
    "    # When the batch is full or at the last record, save and reset\n",
    "    if len(batch_X) == BATCH_SIZE or i == len(ptbxl_records):\n",
    "        batch_X = np.stack(batch_X).astype(np.float32)\n",
    "        fname = f'ptbxl_batch_{batch_id}.npy'\n",
    "        np.save(fname, batch_X)\n",
    "        temp_files.append(fname)\n",
    "        print(f'Saved {fname}  shape {batch_X.shape}')\n",
    "        batch_X = []\n",
    "        batch_id += 1\n",
    "\n",
    "# Concatenate all temporary batches with minimal RAM usage\n",
    "X_neg = np.concatenate([np.load(f) for f in temp_files])\n",
    "y_neg = np.zeros(len(X_neg), dtype=bool)\n",
    "print(f'Final PTB-XL array shape: {X_neg.shape}')\n",
    "\n",
    "# Save the full processed PTB-XL set\n",
    "np.savez_compressed('ptbxl_proc.npz', X=X_neg, y=y_neg)\n",
    "print('Saved ptbxl_proc.npz')\n",
    "\n",
    "# Remove temporary files\n",
    "for f in temp_files:\n",
    "    os.remove(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95073a41",
   "metadata": {},
   "source": [
    "### Combine external test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28186aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External test shape: (23430, 2920, 12), positives: 1631\n",
      "Saved test_external.npz\n"
     ]
    }
   ],
   "source": [
    "# Load processed positive and negative arrays\n",
    "pos_data = np.load('samitrop_proc.npz')\n",
    "neg_data = np.load('ptbxl_proc.npz')\n",
    "\n",
    "# Combine the two datasets into a single external test set\n",
    "X_test = np.concatenate([pos_data['X'], neg_data['X']])\n",
    "y_test = np.concatenate([pos_data['y'], neg_data['y']])\n",
    "\n",
    "print(f'External test shape: {X_test.shape}, positives: {y_test.sum()}')\n",
    "\n",
    "# Save the combined external test set\n",
    "np.savez_compressed('test_external.npz', X=X_test, y=y_test)\n",
    "print('Saved test_external.npz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds207-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
